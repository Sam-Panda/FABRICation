{"cells":[{"cell_type":"markdown","id":"5c2150eb","metadata":{},"source":["# Master notebook\n","#### This is the master notebook which contains the code to process the data from the shortcut and call the child notebooks to create the temporary delta tables. The code contains the following steps:\n","1. List the files in the input directory.\n","2. Filters the files which are required to be processed.\n","3. Group the files based on the number of jobs that can be run in parallel.\n","4. Call the child notebooks to process the data in parallel.\n","5. Merge the temporary delta tables to create the final delta table.\n","6. Save the final delta table as a table in the Lakehouse."]},{"cell_type":"code","execution_count":null,"id":"916c8153-56d4-47e9-b405-493646430871","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[],"source":["# declaring the variables\n","input_file_path = f\"Files/nycyellotaxi-backup\"\n","output_path = \"Files/parquet-to-delta-table-fabric\"\n","keywords_to_be_considered =['2022','2021','2020','2019']\n","no_of_parallel_jobs = 6\n"]},{"cell_type":"code","execution_count":null,"id":"6e097be8-6e44-45f6-8ae8-d2a7699eafcc","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Listing the files in the  shortcut path\n","from notebookutils import mssparkutils\n","_input_files_path =   mssparkutils.fs.ls(f\"{input_file_path}\")\n","input_files_path = []\n","for fileinfo in _input_files_path:\n","    input_files_path.append(fileinfo.path)\n","\n","# Filtering the files based on the keywords.\n","files_path = []\n","filtered_list_of_path = []\n","for keyword in keywords_to_be_considered:\n","    filtered_list_of_path = [i for i in input_files_path if keyword in i]\n","    for f in filtered_list_of_path:\n","        files_path.append(f)"]},{"cell_type":"code","execution_count":null,"id":"bfd82d84-4eb9-4d65-8744-90d4d043693b","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["len(files_path)"]},{"cell_type":"code","execution_count":null,"id":"2749d94b-b86c-4a96-84f6-9d76ce05dff0","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Group the files based on the number of jobs that can be run in parallel.\n","\n","def chunkIt(seq, num):\n","    avg = len(seq) / float(num)\n","    out = []\n","    last = 0.0\n","\n","    while last < len(seq):\n","        out.append(seq[int(last):int(last + avg)])\n","        last += avg\n","\n","    return out"]},{"cell_type":"code","execution_count":null,"id":"200ccadd-9a4b-42a5-9ceb-37237d47a452","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["files_list_part = chunkIt(filtered_list_of_path, no_of_parallel_jobs)"]},{"cell_type":"code","execution_count":null,"id":"7c28ede9-d4c4-4d51-9737-de0bd4fec6bb","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# creating the list of notebooks to be executed in parallel with the parameters that are required for the child notebook.\n","notebooks = []\n","for i in range(0, no_of_parallel_jobs):\n","    notebook = {\"path\": \"/child_notebook_parallelism\", \"params\": {\"files_list_part\": f\"{files_list_part[i]}\", \"output_path\" : f\"{output_path}/temp/batch{i}\"}}\n","    notebooks.append(notebook)"]},{"cell_type":"code","execution_count":null,"id":"19ac931d-0043-4f6f-9745-92bb75aa265c","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# execute the child notebooks in parallel\n","from concurrent.futures import ThreadPoolExecutor\n","timeout = 1800 # 3600 seconds = 1 hour\n","# notebooks = [\n","#     {\"path\": \"/childnotebook\", \"params\": {\"files_list_part\": f\"{files_list_part[0]}\", \"output_path\" : f\"{output_path}/temp/batch0\"}},\n","#     {\"path\": \"/childnotebook\", \"params\": {\"files_list_part\": f\"{files_list_part[1]}\", \"output_path\" : f\"{output_path}/temp/batch1\"}},\n","#     {\"path\": \"/childnotebook\", \"params\": {\"files_list_part\": f\"{files_list_part[2]}\", \"output_path\" : f\"{output_path}/temp/batch2\"}},\n","# ]\n","\n","with ThreadPoolExecutor() as ec:\n","    for notebook in notebooks:\n","        f = ec.submit(mssparkutils.notebook.run, notebook[\"path\"], timeout, notebook[\"params\"])\n","        # print(f\"notebook-path - {notebook['path']}\")\n","        # print(f\"notebook-params - {notebook['params']}\")"]},{"cell_type":"code","execution_count":null,"id":"aef8953b-6880-4e66-afdc-6234a88cc935","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["table_delta_file_location = f\"Tables/test2_merge\"\n","table_full_name = \"test2_merge\"\n","merge_join_condition = \"source.hash_key = target.hash_key\""]},{"cell_type":"code","execution_count":null,"id":"b3888cfe-7fca-4d8d-96ae-9519b1a07c2f","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["def create_delta_table(\n","    df,\n","    table_full_name,\n","    table_delta_file_location\n","):\n","    isDeltaTableAlreadyPresent = 0\n","    try:\n","        mssparkutils.fs.ls(table_delta_file_location)\n","        isDeltaTableAlreadyPresent = 1\n","    except:\n","    #writing the delta table into the curated location\n","        df.write.format(\"delta\").mode(\"overwrite\").save(table_delta_file_location)\n","        sqltext = f\"CREATE TABLE IF NOT EXISTS {table_full_name} USING DELTA LOCATION '{table_delta_file_location}'\"\n","        # print(sqltext)\n","        spark.sql(sqltext)\n","    return  isDeltaTableAlreadyPresent"]},{"cell_type":"code","execution_count":null,"id":"56561a04-8aaf-49b5-a965-f3ad304187cb","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["def mergeDeltaTable(\n","    table_full_name,\n","    df,\n","    merge_join_condition\n","):\n","    df.createOrReplaceTempView(\"temp_vw_new_data\")\n","    sqltext = (f'''\n","\n","    MERGE INTO {table_full_name} as source\n","    USING temp_vw_new_data as target\n","    ON {merge_join_condition}    \n","    WHEN MATCHED THEN UPDATE SET *\n","    WHEN NOT MATCHED THEN INSERT *\n","\n","    \n","    \n","    ''')\n","    # print(sqltext)\n","    spark.sql(sqltext)"]},{"cell_type":"markdown","id":"c61070c8","metadata":{},"source":["## The below code is merging the delta tables that are being created by the child notebooks in the above parallel execution.\n"]},{"cell_type":"code","execution_count":null,"id":"1be741ba-fc3c-406e-ab19-c858e113c0c1","metadata":{"advisor":{"adviceMetadata":"{\"artifactId\":\"145f48da-1f66-4f45-bb9a-31693ef0238b\",\"activityId\":\"5c0dc1cb-2adb-4178-9c1b-acb531417d3e\",\"applicationId\":\"application_1716199935812_0001\",\"jobGroupId\":\"13\",\"advices\":{\"warn\":2}}"},"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import time\n","import timeit\n","import functools\n","from pyspark.sql import DataFrame\n","from pyspark.sql.functions import expr, col\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import row_number\n","\n","start_time = time.time()\n","output_dfs = []\n","# reading all the output parquet files from the parallel jobs output\n","for i in range(0, no_of_parallel_jobs):\n","    input_path = f\"{output_path}/temp/batch{i}\"\n","    df = spark.read.parquet(input_path)\n","    output_dfs.append(df)\n","\n","# union all the dataframes into one dataframe\n","df_output = functools.reduce(DataFrame.unionAll, output_dfs)\n","# removing the duplicates from the dataframe\n","windowSpec = Window.partitionBy(\"hash_key\").orderBy(\"hash_key\")\n","df_output = df_output.withColumn(\"row_num\", row_number().over(windowSpec)).filter(\"row_num=1\")\n","df_output = df_output.drop(\"row_num\")\n","# creating the delta table if it is not present\n","isDeltaTableAlreadyPresent = create_delta_table(\n","        df=df_output,\n","        table_full_name=table_full_name,\n","        table_delta_file_location=table_delta_file_location\n",")\n","print (f\"isDeltaTableAlreadyPresent = {isDeltaTableAlreadyPresent} [[ 0= Not Present, so we created the delta table. 1= present ]], we skip creation of the delta table\")\n","\n","# merging the new dataframe with the delta table\n","if (isDeltaTableAlreadyPresent==1):\n","    print(\" We are going to merge the new dataframe with the delta table\")\n","    mergeDeltaTable(\n","            merge_join_condition=merge_join_condition,\n","            df=df_output,\n","            table_full_name=table_full_name\n","    )\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","print(f\"Execution time: {elapsed_time}\")"]},{"cell_type":"code","execution_count":null,"id":"5c2efaba-cf13-4c80-a65d-9578725759e7","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["%%sql\n","\n","select count(*) from test2_merge"]}],"metadata":{"dependencies":{"environment":{"environmentId":"7c294e3a-7890-4fcd-ab63-8a666f98990a","workspaceId":"2aec5ec0-be39-4bf7-b0bf-9c71a008a4e3"},"lakehouse":{"default_lakehouse":"eb2a735e-9dac-4a8d-aed0-e112d333b768","default_lakehouse_name":"LH1","default_lakehouse_workspace_id":"2aec5ec0-be39-4bf7-b0bf-9c71a008a4e3"}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"state":{},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
